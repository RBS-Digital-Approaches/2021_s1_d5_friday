{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2021_s1_d5_m2_image_comparison.ipynb","provenance":[{"file_id":"1BlKp2nueAHvFCBUB-v2o2adTvB3sYJrv","timestamp":1626302866387}],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPwtdU738FccwgeSJgGaTtj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"zd-FCo-G71ST"},"source":["# Image Matching Overview"]},{"cell_type":"markdown","metadata":{"id":"bdtcNSf83ijL"},"source":["In this module we will experiment with two domain of computational image analysis:  1) Comparison and Classification.  Matching describes the domain of effort where we are primarily concerned with computationally determining the degree to which any two images are the same.  For example, we may have a copy of a woodblock print that have found in one collection and want to know if other copies of the same print exist in other collections. \n","\n","The Classification domain is primarily focused on the degree to which images are similar to each other.  For example, we want to train a computer to search through large image catalogues and automatically separate all photographs of paintings from photographs of digital prints.  The OCR processes with which you have already worked are a good example of a classification problems.  The letter “D” appears in many fonts and sizes, many of which differ significantly, but as humans we still recognize them as belong to category (or class) of “D.”\n","\n","The drove the to deploy computational methods in both domains is driving largely by the problem of scale and the cognitive difficulty that we, as human, encounter when trying to work with very large image collections—a problem that has amplified in this era of mass digitization.  But here, as elsewhere, we should not overlook the value that computers can bring to our work as intelligent provocateurs.  When we deploy computational methods, we frequently get seemingly bizarre and incorrect results.  However, we should not be so quick dismiss results that we fail to understand.  \n","\n","Computers are, if nothing else, thorough and consistent, and the results that they produce are not arbitrary.  As such, we should seriously consider and work, when possible, to understand why they draw the conclusions they do, even when they seem nonsensical, as doing so often leads to new insights about the bias that we bring to our own research questions.\n"]},{"cell_type":"markdown","metadata":{"id":"04DbqCkQ8OLB"},"source":["# Interpretability"]},{"cell_type":"markdown","metadata":{"id":"XBcgatVuAWd3"},"source":["All machine learning process are of two primary types:   Interpretable and uninterpretable.  Interpretable models are a class of model that expose the processes behind and reasons why they produce the final results that they produce.  Interpretable models frequently do not perform as well as their non-interpretable counterparts; however, their interpretability offers significant value to humanities scholars, as the process of working to interpret process behind the results achieved is playing field of intellectual provocation and response through which we can advance our own understanding of the artifacts we study.\n","\n","When an interpretable model presents an unexpected result, we can investigate why, and learn something in the process.  When an uninterpretable process returns an unexpected result, all we do is call it wrong and move on.  \n","\n","In this module, we will focus on interpretable models—one each in each of the domains of comparison and classification.  This notebook provides ready to run code, so you will not be required to do any heavy lifting on the coding front.  Rather, you are encouraged to run each process multiple times on multiple sets of images in order to see how they perform.  In each case, as you work, try to determine why the computer is drawing the conclusion it is drawing, rather than simply assessing whether you believe the results are right or wrong.  When we convene as a group, we will discuss our insights while trying to understand how the computer thinks."]},{"cell_type":"markdown","metadata":{"id":"9JbO71nBAeVH"},"source":["# Environment Setup"]},{"cell_type":"markdown","metadata":{"id":"J__iCigxAhff"},"source":["First we'll install all required packages and modules into the environment."]},{"cell_type":"code","metadata":{"id":"i2snUk2s6VvE"},"source":["# install the scikit-learn package\n","!pip install scikit-learn"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pJRvJDEbSyH2"},"source":["# install the scikit-image package\n","!pip install scikit-image"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pbw5ZDk3ZhO-"},"source":["Now we import the packages we need into the environement."]},{"cell_type":"code","metadata":{"id":"Wb8zN-TNKvuG","executionInfo":{"status":"ok","timestamp":1626415088940,"user_tz":420,"elapsed":1279,"user":{"displayName":"Carl Stahmer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjie_9PMt5uEdSFL1nVTqBVH78Dsg4mVdhYbYUFww=s64","userId":"08270031735613254632"}}},"source":["import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import scipy\n","import networkx as nx\n","import matplotlib\n","import pytest\n","import random\n","import sklearn\n","import os"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B9HRffmrZcME"},"source":["Mount the Google Drive."]},{"cell_type":"code","metadata":{"id":"LPmDDjSmP0bx"},"source":["from google.colab import drive\n","drive.mount('/gdrive/')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"THRNKe1hZocc"},"source":["Here we define an input file path in the class_data repo which contains several hundred images of woodcut impressions from he English Broadside Ballad Archive collection as well as an output directory will save files.  These directories will be reference by both sections of the notebook."]},{"cell_type":"code","metadata":{"id":"tdudVAMrQIsR","executionInfo":{"status":"ok","timestamp":1626415131659,"user_tz":420,"elapsed":164,"user":{"displayName":"Carl Stahmer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjie_9PMt5uEdSFL1nVTqBVH78Dsg4mVdhYbYUFww=s64","userId":"08270031735613254632"}}},"source":["in_path = \"/gdrive/MyDrive/rbs_digital_approaches_2021/data_class/ebba_woodblocks/\"\n","out_path = \"/gdrive/MyDrive/rbs_digital_approaches_2021/output/\""],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KEvkR7ToeaPk"},"source":["# Feature Identification and Extraction"]},{"cell_type":"markdown","metadata":{"id":"CO4RO_MiejDf"},"source":["In this section of the notebook we will explore basic image matching techniques.  Contrary to popular expectation, image mapping is rarely accomplished by comparing the shape(s) of object.  Shape matching (contour matching, to image specialists) is an extremely faulty process because slight changes in orientation (such as those produced by changes in the orientation, height, or lens type of the camera taking the image have impacts on the shapes of things as they appear in images, as we’ve already discussed in class. \n","\n","One method of image comparison that can work quite well with modern, color photography is color histogram analysis.  With this process, each pixel in an image is examined and assigned a numeric score based on the color present in the pixel.  This creates a matrix representation of the image of the same dimensions as the pixel dimensions of the image.  This matrix can be compared with other matrices for similarity as a means of finding a matching image.  \n","\n","As noted, this approach works well on color images with high contrast and a lot of objects pictured in them.  But it does not work well on black and white images, which comprise a great deal of the body of digitally available historical materials.  They also fail on high resolution color images of historical printed materials because both the structure and color of the paper on which they were printed changes from copy to copy, which changes the color score of the pixels.\n","\n","The preferred method of performing interpretable, computational image mapping is known as Bag of Features matcing. We’ll explain the approach *in situ* as we work through its application below.\n"]},{"cell_type":"markdown","metadata":{"id":"uiVt1_7dl-n8"},"source":["First, we'll load two images to compare from the previously defined working diectory.  Please take a moment to use the Google Drive web interface to naviagate to the './data_class/ebba_woodblocks/ dirctory in yor google drive  amd look at the photos we'll be using."]},{"cell_type":"code","metadata":{"id":"EhA6gU39QDcF","executionInfo":{"status":"ok","timestamp":1626415141707,"user_tz":420,"elapsed":5657,"user":{"displayName":"Carl Stahmer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjie_9PMt5uEdSFL1nVTqBVH78Dsg4mVdhYbYUFww=s64","userId":"08270031735613254632"}}},"source":["# Load the first Image\n","img_file_name = \"rox-2-564-565.jpg\"\n","img_file = in_path + img_file_name\n","img = cv2.imread(img_file, cv2.IMREAD_GRAYSCALE)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"IY0-uYeLX8Im","executionInfo":{"status":"ok","timestamp":1626415144085,"user_tz":420,"elapsed":294,"user":{"displayName":"Carl Stahmer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjie_9PMt5uEdSFL1nVTqBVH78Dsg4mVdhYbYUFww=s64","userId":"08270031735613254632"}}},"source":["# Load the second Image\n","img2_file_name = \"rox-1-100-101.jpg\"\n","img_file = in_path + img2_file_name\n","img2 = cv2.imread(img_file, cv2.IMREAD_GRAYSCALE)"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q5yH6eXdwZc3"},"source":["Note that in te code above, not only did we load the images, but we also converted them to greyschale.  This is because the color information in the photo has no impact on feature identification but does require significanly computational power.  Cost without benefit is never goos=d, so we simply remove the color informaion."]},{"cell_type":"markdown","metadata":{"id":"8EQm9rIbmr9t"},"source":["Now that we've identified and loaded two images for comparison, we need to declare an instance the ORB_create feature extraction class form the Pthon OpenCV package.  \n","\n","The Bag of Features approach discards the idea that images depict discreet objects (a picture of a person and a car, for example) in favor of the idea that they are really just containers for collections of small, mathematically definable angles.   \n","\n","To perform Bag of Features matching, for each image in a collection the computer first decomposes (blurs and the intensifies contrast) the image and then locates any lines (contours and edges) that appear in it.  It then looks for any changes in direction in line segments that create measurable and calculable angles.  These locations in the image are called \"keypoints.\"  It then records location of each keuypoint along with its mathematial defintion in matrix form.  The keypoint and its description, combined, are called a feature.  And the matrix that stores all the features found in an image is known as a Bag of Features.\n","\n","[OpenCV](https://opencv.org/) is a long-running, opensource image recognition platform, written in C++ that can be invoked through a Python wrapper.  And ORB is one of many feature extraction algorithms that can be run in OpenCV.  The most widely used and highest functioning feature extraction algorithms are The SURF and SIFT algorithms.  Unfortunately, because both are patent protected, they are not allowed to be used in Google Collaborator.  As such, we have defaulted to the opensource ORB algorithm.\n"]},{"cell_type":"code","metadata":{"id":"BtHu99loQ1s6","executionInfo":{"status":"ok","timestamp":1626415150911,"user_tz":420,"elapsed":167,"user":{"displayName":"Carl Stahmer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjie_9PMt5uEdSFL1nVTqBVH78Dsg4mVdhYbYUFww=s64","userId":"08270031735613254632"}}},"source":["# create a feature extraction object\n","# that uses open-source ORB features.\n","orb = cv2.ORB_create()"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jx7ED8s1pegh"},"source":["When performing feature extraction, you must tell the algorithm how many \"levels\" of extraction to perform.  Levels can be thought of as passed, where after each single pass and before the next a blurring filter is applied to the image.  This has the effect of gradually eroding smaller features and expanding larger one.  Here, we set the value to 3.  You should play around with this number and run different extractions to see how it affects the resulting features and matching."]},{"cell_type":"code","metadata":{"id":"O40rsH_0pYUG","executionInfo":{"status":"ok","timestamp":1626415154526,"user_tz":420,"elapsed":132,"user":{"displayName":"Carl Stahmer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjie_9PMt5uEdSFL1nVTqBVH78Dsg4mVdhYbYUFww=s64","userId":"08270031735613254632"}}},"source":["# determine the number of levels\n","orb.setNLevels(3)"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VgK2CDVNqa_M"},"source":["New we call the \"detectAndCompute\" function to genrate keypoints and descriptors for both of our test images."]},{"cell_type":"code","metadata":{"id":"qHvG-o7fRNui","executionInfo":{"status":"ok","timestamp":1626415161040,"user_tz":420,"elapsed":303,"user":{"displayName":"Carl Stahmer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjie_9PMt5uEdSFL1nVTqBVH78Dsg4mVdhYbYUFww=s64","userId":"08270031735613254632"}}},"source":["keypoints, descriptors = orb.detectAndCompute(img, None)\n","keypoints2, descriptors2 = orb.detectAndCompute(img2, None)"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yA-i7A4Zrc1C"},"source":["In the code cells below, we draw the features found on the first of our test images onto the image itself and save it to the Google Drive in the “output” directory of our course home directory. After you run the cell, look at the image to get an idea of where the computer has located features. Then, try a few more images. And then try the same images with different tunings.\n","\n","When features are drawn on an image, the size of the circle represents the size of the feature.  And the line tha extends from center to circumference shows the direction the feature angle is facing."]},{"cell_type":"code","metadata":{"id":"IY-gpqnwRaEL","executionInfo":{"status":"ok","timestamp":1626415166131,"user_tz":420,"elapsed":144,"user":{"displayName":"Carl Stahmer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjie_9PMt5uEdSFL1nVTqBVH78Dsg4mVdhYbYUFww=s64","userId":"08270031735613254632"}}},"source":["# create the new image with kehpoints drawn on it\n","img_w_featues = cv2.drawKeypoints(img, keypoints, None, None, flags=4)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"7HP_80CrRxnB"},"source":["# write the keypoint image to Google Drive\n","out_img_path = out_path + \"features-9\" + img_file_name\n","cv2.imwrite(out_img_path, img_w_featues )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_Oh-8Sb9zLUV"},"source":["# Direct Comparison Matching"]},{"cell_type":"markdown","metadata":{"id":"bsEh1hbctSZo"},"source":["Now that we've learned how to find, extract, and draw features on an image, it's time to do some image matching.  This is accomplished by finding they keypoints on any two image and then comparing the two sets of keypoints, the more keypoints that match, the more likely that the two images are the same.\n","\n","Here, we need to say a few words about \"sameness.\"  What exactly do we mean by \"same.\"  Do we mean identical?  If so then two images would be the same if, and only if, they were exact copies of each other.  If one were cropped slightly smaller, or if it was of only half of the original sheet, it would not match with a perfect image of the whole sheet.  And what about skew? \n","\n","It turns out, there’s a fair amount of difference in sameness.  The Bag of Features method solves many of the above problems because feature descriptions are both rotation and scale invariant, so neither the rotation nor the scale of images being examined affects the outcome.   \n","\n","The sammple images that are pre-identifided below are designed to illustrate this point.  They are of wooblock impressions made on two diferent broadside ballads, with different surrounding maerial.  Once you run the match once with these images, you should experiment with others."]},{"cell_type":"markdown","metadata":{"id":"alfUDGg-wC7P"},"source":["The first step in the matching process is to create an instance of the needed matcher class.  "]},{"cell_type":"code","metadata":{"id":"3iweTuZRZLG1","executionInfo":{"status":"ok","timestamp":1626415209662,"user_tz":420,"elapsed":134,"user":{"displayName":"Carl Stahmer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjie_9PMt5uEdSFL1nVTqBVH78Dsg4mVdhYbYUFww=s64","userId":"08270031735613254632"}}},"source":["# create BFMatcher object\n","bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0wgFj-b7xIzA"},"source":["Now that we have a matcher ready, we send it the two sets of feature descriptions for our two images and let it look for matches."]},{"cell_type":"code","metadata":{"id":"_cXW4xzTZSK8","executionInfo":{"status":"ok","timestamp":1626415214047,"user_tz":420,"elapsed":156,"user":{"displayName":"Carl Stahmer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjie_9PMt5uEdSFL1nVTqBVH78Dsg4mVdhYbYUFww=s64","userId":"08270031735613254632"}}},"source":["# Match descriptors.\n","matches = bf.match(descriptors, descriptors2)"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2klE_RnfxZqj"},"source":["We'll now sort the matches based on their mathematic similarity.  This will allow us to look at the most significant one."]},{"cell_type":"code","metadata":{"id":"vjAg8uTzZcaS","executionInfo":{"status":"ok","timestamp":1626415217009,"user_tz":420,"elapsed":127,"user":{"displayName":"Carl Stahmer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjie_9PMt5uEdSFL1nVTqBVH78Dsg4mVdhYbYUFww=s64","userId":"08270031735613254632"}}},"source":["# Sort them in the order of their distance.\n","matches = sorted(matches, key = lambda x:x.distance)"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZDoR_aU4xnE_"},"source":["Now, we use the drawMatches function to draw the matches found between our two images on a new image."]},{"cell_type":"code","metadata":{"id":"5j6g4r03ZhAK","executionInfo":{"status":"ok","timestamp":1626415220843,"user_tz":420,"elapsed":127,"user":{"displayName":"Carl Stahmer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjie_9PMt5uEdSFL1nVTqBVH78Dsg4mVdhYbYUFww=s64","userId":"08270031735613254632"}}},"source":["# Draw first 50 matches.\n","image_w_matches = cv2.drawMatches(img, keypoints, img2, keypoints2, matches[:50], None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bt55twT0x1JJ"},"source":["And now that we've drawn a match image, we'll save it to disk for viewing."]},{"cell_type":"code","metadata":{"id":"eIanqQDwaC1h"},"source":["out_img_path2 = out_path + \"rox-9matches.jpg\"\n","cv2.imwrite(out_img_path2, image_w_matches )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AsY2aiY0yNAJ"},"source":["Drawing the matches on an image for viewing is useful for teaching, but not very usefull in most real-worls use cases where we want to computatational act on our similiarty finding.  In this context, we would normally run something like the code below to get a mathematical assessment of similarit and the act accordingly."]},{"cell_type":"code","metadata":{"id":"vyyrk6qZQAud"},"source":["compare = cv2.batchDistance()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LCs9yK-iinCs"},"source":["# Matching One to Many"]},{"cell_type":"markdown","metadata":{"id":"xfoR_lzJzuCu"},"source":["In this section of the notebook we'll work on matching across a larger image library.  In the previous section, we used a brute-force matching technique that returned positive only if a direct match of features was found.  In this section, we’ll use Cosine Similarity to give us a similarity ranking for each image."]},{"cell_type":"markdown","metadata":{"id":"98HWTFbE0llP"},"source":["To start, we'll need to get a directory listing/ filelist for our input directory.  "]},{"cell_type":"code","metadata":{"id":"PlpvlbFtitoY","executionInfo":{"status":"ok","timestamp":1626415285905,"user_tz":420,"elapsed":125,"user":{"displayName":"Carl Stahmer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjie_9PMt5uEdSFL1nVTqBVH78Dsg4mVdhYbYUFww=s64","userId":"08270031735613254632"}}},"source":["file_list = os.listdir(in_path) "],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Frw8yjdj669j"},"source":["Now, we'll run our feature extraction on all images in the input directory.  To do this, we create an ORB object, setup some emtpy lists to hold dat sanpshots, then loop through each image in the directory and exract its features.  Note that running this cell could take several minutes."]},{"cell_type":"code","metadata":{"id":"ezxXgVxM68-7","executionInfo":{"status":"ok","timestamp":1626415372731,"user_tz":420,"elapsed":80779,"user":{"displayName":"Carl Stahmer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjie_9PMt5uEdSFL1nVTqBVH78Dsg4mVdhYbYUFww=s64","userId":"08270031735613254632"}}},"source":["# define a feature point extractor\n","orb = cv2.ORB_create()\n","orb.setNLevels(5)\n","\n","# create and empty list to put features into\n","features_list = []\n","features_dict = dict()\n","image_names = []\n","image_descriptors = []\n","# loop through files and process\n","for next_file in file_list:\n","  # define the file path\n","  next_path = in_path + next_file\n","  # define a size for the descriptors vector\n","  needed_size = 50000\n","\n","  # read in the file\n","  img = cv2.imread(next_path, cv2.IMREAD_GRAYSCALE)\n","  # calculate the keypoints\n","  #keypoints, descriptors = orb.detectAndCompute(img, None)\n","\n","  # compute kepoints\n","  kps = orb.detect(img)\n","\n","  # only keeping the largest 32 keypoints of each image\n","  kps = sorted(kps, key=lambda x: -x.response)[:32]\n","\n","  # compute the descriptors vector\n","  kps, dsc = orb.compute(img, kps)\n","  # Flatten all of them in one big vector - our feature vector\n","  dsc = dsc.flatten()\n","  # Making descriptor of same size as Descriptor vector size is 64\n","  if dsc.size < needed_size:\n","    # if we have less the 32 descriptors then just adding zeros at the\n","    # end of our feature vector\n","     dsc = np.concatenate([dsc, np.zeros(needed_size - dsc.size)])\n","\n","    # reshape the vector\n","    #dsc.reshape(1, -1)\n","\n","  # make a list that associates the filename with the keypoints\n","  temp_list = [next_file, dsc]\n","  # add the single image data to the list of lists\n","  features_list.append(temp_list)\n","  features_dict.update({next_file: dsc})\n","\n","  image_names.append(next_file)\n","  image_descriptors.append(dsc)"],"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7u498ZcMSi-W"},"source":["We'll designate the file we want to use as our seed here.  This image will be compared to all other images.  "]},{"cell_type":"code","metadata":{"id":"rrGGN4FuS9LR","executionInfo":{"status":"ok","timestamp":1626415442675,"user_tz":420,"elapsed":142,"user":{"displayName":"Carl Stahmer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjie_9PMt5uEdSFL1nVTqBVH78Dsg4mVdhYbYUFww=s64","userId":"08270031735613254632"}}},"source":["# first set an image you want to test from\n","# the set of processed images\n","test_image = \"20021-10.jpg\"\n","\n","# get the index for this image\n","test_index = image_names.index(test_image)\n","\n","# get the descriptors for this image\n","test_descriptor = image_descriptors[test_index]"],"execution_count":23,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QvPRLh-T2pRR"},"source":["Now we loop through ever other image in the dataset and calculate the similarity score between the two images.  Note the commented out \"np.dot\" line in the for loop.  Dot Multiplication is another statistical method for assessing the similarity between vectors. If you have time, switch between the two siilarity calculations and see what effect it has on the outcome."]},{"cell_type":"code","metadata":{"id":"1QenmHwnYMlE","executionInfo":{"status":"ok","timestamp":1626415500417,"user_tz":420,"elapsed":342,"user":{"displayName":"Carl Stahmer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjie_9PMt5uEdSFL1nVTqBVH78Dsg4mVdhYbYUFww=s64","userId":"08270031735613254632"}}},"source":["# create a list of comparision scores for this\n","# image against all images in set\n","sim_list = []\n","for i in range(len(image_descriptors)):\n","  #sim = np.dot(image_descriptors[test_index], image_descriptors[i])\n","  sim = scipy.spatial.distance.cosine(image_descriptors[test_index], image_descriptors[i])\n","  sim_list.append(sim)"],"execution_count":24,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UgffC7b_3XaU"},"source":["Now that we've calculated all the similarity scores, we'll remove the original seed image from the dataset, otherwise we'd be comparing it to itself."]},{"cell_type":"code","metadata":{"id":"CdZCa2sCSory","executionInfo":{"status":"ok","timestamp":1626415505776,"user_tz":420,"elapsed":134,"user":{"displayName":"Carl Stahmer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjie_9PMt5uEdSFL1nVTqBVH78Dsg4mVdhYbYUFww=s64","userId":"08270031735613254632"}}},"source":["# remove test image from the data\n","img_names_temp = image_names\n","sim_list_temp = sim_list\n","del img_names_temp[test_index]\n","del sim_list_temp[test_index]\n"],"execution_count":25,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_ywOcmz63hsv"},"source":["We also have to do some data munging.  We need to sort our results so that when we plot them they will be more readable.  The best way to do this is to take our list of image names and our list of similarity scores and join them as a dictionay, that way we can sort without loosing name/score associations."]},{"cell_type":"code","metadata":{"id":"Yv8V8dEsgHLk","executionInfo":{"status":"ok","timestamp":1626415509180,"user_tz":420,"elapsed":137,"user":{"displayName":"Carl Stahmer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjie_9PMt5uEdSFL1nVTqBVH78Dsg4mVdhYbYUFww=s64","userId":"08270031735613254632"}}},"source":["# join to lists as a dictionary\n","sim_dict = {img_names_temp[i]: sim_list_temp[i] for i in range(len(sim_list_temp))}\n","# now sort the dictionary\n","sim_dict = dict(sorted(sim_dict.items(), key=lambda x: x[1], reverse=True))"],"execution_count":26,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NSXTIrt2wO-1"},"source":["First, we'll plot the similarity scores for the entire dataset."]},{"cell_type":"code","metadata":{"id":"CIyE5QPewVkt"},"source":["import matplotlib.pyplot as plt\n","# set the size of the plot\n","fig = plt.figure(figsize=(12, 9))\n","ax = fig.add_axes([0,0,1,1])\n","ax.bar(sim_dict.keys(), sim_dict.values())\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2iJGSEFH4YaQ"},"source":["The above visualization is useful for giving us an overall idea of shape of our data, but there are too many datapoints for the graph to be truly legible.  To solve this, let's subset the results and only look at the top most related images."]},{"cell_type":"code","metadata":{"id":"j0Bui-u2sw84","executionInfo":{"status":"ok","timestamp":1626415531092,"user_tz":420,"elapsed":150,"user":{"displayName":"Carl Stahmer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjie_9PMt5uEdSFL1nVTqBVH78Dsg4mVdhYbYUFww=s64","userId":"08270031735613254632"}}},"source":["# subset first 20 items of list for plotting\n","sim_dict_subset = dict(list(sim_dict.items())[0: 10]) "],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"KSzqqWugceKQ"},"source":["# draw the plot\n","import matplotlib.pyplot as plt\n","fig = plt.figure(figsize=(12, 9))\n","ax = fig.add_axes([0,0,1,1])\n","ax.bar(sim_dict_subset.keys(), sim_dict_subset.values())\n","plt.show()"],"execution_count":null,"outputs":[]}]}